{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0ec8165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 가져오기\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes_data = load_diabetes()\n",
    "\n",
    "df_X = diabetes_data.data\n",
    "df_y = diabetes_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c16fb166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990842\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06832974\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286377\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04687948\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452837\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00421986\n",
      "   0.00306441]]\n"
     ]
    }
   ],
   "source": [
    "# 모델에 입력할 데이터 X 준비하기\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(df_X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f236a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n"
     ]
    }
   ],
   "source": [
    "# 모델에 예측할 데이터 y 준비하기\n",
    "y = np.array(df_y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08022bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터와 test 데이터로 분리하기\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3c50190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 1 : age\n",
      "feature 2 : sex\n",
      "feature 3 : bmi\n",
      "feature 4 : bp\n",
      "feature 5 : s1\n",
      "feature 6 : s2\n",
      "feature 7 : s3\n",
      "feature 8 : s4\n",
      "feature 9 : s5\n",
      "feature 10 : s6\n",
      "        age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
      "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
      "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
      "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
      "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
      "\n",
      "         s4        s5        s6  \n",
      "0 -0.002592  0.019908 -0.017646  \n",
      "1 -0.039493 -0.068330 -0.092204  \n",
      "2 -0.002592  0.002864 -0.025930  \n",
      "3  0.034309  0.022692 -0.009362  \n",
      "4 -0.002592 -0.031991 -0.046641  \n"
     ]
    }
   ],
   "source": [
    "# 모델 준비하기\n",
    "for i, feature_name in enumerate(diabetes.feature_names) :\n",
    "    print(f'feature {i+1} : {feature_name}')\n",
    "    \n",
    "df = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "print(df[['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54a120aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 loss 정의하기\n",
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "\n",
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions\n",
    "\n",
    "def MSE(a, b):\n",
    "    mse = ((a-b)**2).mean()\n",
    "    return mse\n",
    "\n",
    "def loss(X, W, b, y):\n",
    "    predictions = model(X, W, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60eb8156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기를 구하는 gradient 함수 구현하기\n",
    "def gradient(X, W, b, y):\n",
    "    N = len(y)\n",
    "    \n",
    "    y_pred = model(X, W, b)\n",
    "    \n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "    \n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb4ece03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터인 학습률 설정하기\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "48d951b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 3571.7273\n",
      "Iteration 20 : Loss 3570.8341\n",
      "Iteration 30 : Loss 3569.9427\n",
      "Iteration 40 : Loss 3569.0531\n",
      "Iteration 50 : Loss 3568.1652\n",
      "Iteration 60 : Loss 3567.2790\n",
      "Iteration 70 : Loss 3566.3946\n",
      "Iteration 80 : Loss 3565.5119\n",
      "Iteration 90 : Loss 3564.6310\n",
      "Iteration 100 : Loss 3563.7518\n",
      "Iteration 110 : Loss 3562.8743\n",
      "Iteration 120 : Loss 3561.9985\n",
      "Iteration 130 : Loss 3561.1244\n",
      "Iteration 140 : Loss 3560.2520\n",
      "Iteration 150 : Loss 3559.3814\n",
      "Iteration 160 : Loss 3558.5124\n",
      "Iteration 170 : Loss 3557.6451\n",
      "Iteration 180 : Loss 3556.7796\n",
      "Iteration 190 : Loss 3555.9157\n",
      "Iteration 200 : Loss 3555.0535\n",
      "Iteration 210 : Loss 3554.1929\n",
      "Iteration 220 : Loss 3553.3340\n",
      "Iteration 230 : Loss 3552.4768\n",
      "Iteration 240 : Loss 3551.6213\n",
      "Iteration 250 : Loss 3550.7674\n",
      "Iteration 260 : Loss 3549.9151\n",
      "Iteration 270 : Loss 3549.0645\n",
      "Iteration 280 : Loss 3548.2156\n",
      "Iteration 290 : Loss 3547.3683\n",
      "Iteration 300 : Loss 3546.5226\n",
      "Iteration 310 : Loss 3545.6785\n",
      "Iteration 320 : Loss 3544.8361\n",
      "Iteration 330 : Loss 3543.9953\n",
      "Iteration 340 : Loss 3543.1561\n",
      "Iteration 350 : Loss 3542.3185\n",
      "Iteration 360 : Loss 3541.4826\n",
      "Iteration 370 : Loss 3540.6482\n",
      "Iteration 380 : Loss 3539.8154\n",
      "Iteration 390 : Loss 3538.9843\n",
      "Iteration 400 : Loss 3538.1547\n",
      "Iteration 410 : Loss 3537.3267\n",
      "Iteration 420 : Loss 3536.5002\n",
      "Iteration 430 : Loss 3535.6754\n",
      "Iteration 440 : Loss 3534.8521\n",
      "Iteration 450 : Loss 3534.0304\n",
      "Iteration 460 : Loss 3533.2103\n",
      "Iteration 470 : Loss 3532.3917\n",
      "Iteration 480 : Loss 3531.5747\n",
      "Iteration 490 : Loss 3530.7592\n",
      "Iteration 500 : Loss 3529.9453\n",
      "Iteration 510 : Loss 3529.1329\n",
      "Iteration 520 : Loss 3528.3221\n",
      "Iteration 530 : Loss 3527.5128\n",
      "Iteration 540 : Loss 3526.7050\n",
      "Iteration 550 : Loss 3525.8987\n",
      "Iteration 560 : Loss 3525.0940\n",
      "Iteration 570 : Loss 3524.2908\n",
      "Iteration 580 : Loss 3523.4891\n",
      "Iteration 590 : Loss 3522.6889\n",
      "Iteration 600 : Loss 3521.8902\n",
      "Iteration 610 : Loss 3521.0931\n",
      "Iteration 620 : Loss 3520.2974\n",
      "Iteration 630 : Loss 3519.5032\n",
      "Iteration 640 : Loss 3518.7105\n",
      "Iteration 650 : Loss 3517.9193\n",
      "Iteration 660 : Loss 3517.1296\n",
      "Iteration 670 : Loss 3516.3413\n",
      "Iteration 680 : Loss 3515.5545\n",
      "Iteration 690 : Loss 3514.7692\n",
      "Iteration 700 : Loss 3513.9854\n",
      "Iteration 710 : Loss 3513.2030\n",
      "Iteration 720 : Loss 3512.4221\n",
      "Iteration 730 : Loss 3511.6426\n",
      "Iteration 740 : Loss 3510.8646\n",
      "Iteration 750 : Loss 3510.0880\n",
      "Iteration 760 : Loss 3509.3129\n",
      "Iteration 770 : Loss 3508.5392\n",
      "Iteration 780 : Loss 3507.7670\n",
      "Iteration 790 : Loss 3506.9962\n",
      "Iteration 800 : Loss 3506.2268\n",
      "Iteration 810 : Loss 3505.4588\n",
      "Iteration 820 : Loss 3504.6922\n",
      "Iteration 830 : Loss 3503.9271\n",
      "Iteration 840 : Loss 3503.1634\n",
      "Iteration 850 : Loss 3502.4011\n",
      "Iteration 860 : Loss 3501.6402\n",
      "Iteration 870 : Loss 3500.8807\n",
      "Iteration 880 : Loss 3500.1226\n",
      "Iteration 890 : Loss 3499.3659\n",
      "Iteration 900 : Loss 3498.6105\n",
      "Iteration 910 : Loss 3497.8566\n",
      "Iteration 920 : Loss 3497.1041\n",
      "Iteration 930 : Loss 3496.3529\n",
      "Iteration 940 : Loss 3495.6031\n",
      "Iteration 950 : Loss 3494.8547\n",
      "Iteration 960 : Loss 3494.1076\n",
      "Iteration 970 : Loss 3493.3619\n",
      "Iteration 980 : Loss 3492.6176\n",
      "Iteration 990 : Loss 3491.8746\n",
      "Iteration 1000 : Loss 3491.1330\n",
      "Iteration 1010 : Loss 3490.3927\n",
      "Iteration 1020 : Loss 3489.6538\n",
      "Iteration 1030 : Loss 3488.9162\n",
      "Iteration 1040 : Loss 3488.1800\n",
      "Iteration 1050 : Loss 3487.4451\n",
      "Iteration 1060 : Loss 3486.7115\n",
      "Iteration 1070 : Loss 3485.9793\n",
      "Iteration 1080 : Loss 3485.2484\n",
      "Iteration 1090 : Loss 3484.5188\n",
      "Iteration 1100 : Loss 3483.7905\n",
      "Iteration 1110 : Loss 3483.0636\n",
      "Iteration 1120 : Loss 3482.3379\n",
      "Iteration 1130 : Loss 3481.6136\n",
      "Iteration 1140 : Loss 3480.8906\n",
      "Iteration 1150 : Loss 3480.1688\n",
      "Iteration 1160 : Loss 3479.4484\n",
      "Iteration 1170 : Loss 3478.7293\n",
      "Iteration 1180 : Loss 3478.0114\n",
      "Iteration 1190 : Loss 3477.2949\n",
      "Iteration 1200 : Loss 3476.5796\n",
      "Iteration 1210 : Loss 3475.8656\n",
      "Iteration 1220 : Loss 3475.1529\n",
      "Iteration 1230 : Loss 3474.4415\n",
      "Iteration 1240 : Loss 3473.7313\n",
      "Iteration 1250 : Loss 3473.0225\n",
      "Iteration 1260 : Loss 3472.3148\n",
      "Iteration 1270 : Loss 3471.6085\n",
      "Iteration 1280 : Loss 3470.9034\n",
      "Iteration 1290 : Loss 3470.1995\n",
      "Iteration 1300 : Loss 3469.4969\n",
      "Iteration 1310 : Loss 3468.7956\n",
      "Iteration 1320 : Loss 3468.0955\n",
      "Iteration 1330 : Loss 3467.3966\n",
      "Iteration 1340 : Loss 3466.6990\n",
      "Iteration 1350 : Loss 3466.0026\n",
      "Iteration 1360 : Loss 3465.3075\n",
      "Iteration 1370 : Loss 3464.6136\n",
      "Iteration 1380 : Loss 3463.9209\n",
      "Iteration 1390 : Loss 3463.2294\n",
      "Iteration 1400 : Loss 3462.5392\n",
      "Iteration 1410 : Loss 3461.8501\n",
      "Iteration 1420 : Loss 3461.1623\n",
      "Iteration 1430 : Loss 3460.4757\n",
      "Iteration 1440 : Loss 3459.7903\n",
      "Iteration 1450 : Loss 3459.1061\n",
      "Iteration 1460 : Loss 3458.4231\n",
      "Iteration 1470 : Loss 3457.7413\n",
      "Iteration 1480 : Loss 3457.0607\n",
      "Iteration 1490 : Loss 3456.3813\n",
      "Iteration 1500 : Loss 3455.7031\n",
      "Iteration 1510 : Loss 3455.0261\n",
      "Iteration 1520 : Loss 3454.3503\n",
      "Iteration 1530 : Loss 3453.6756\n",
      "Iteration 1540 : Loss 3453.0021\n",
      "Iteration 1550 : Loss 3452.3298\n",
      "Iteration 1560 : Loss 3451.6587\n",
      "Iteration 1570 : Loss 3450.9887\n",
      "Iteration 1580 : Loss 3450.3199\n",
      "Iteration 1590 : Loss 3449.6523\n",
      "Iteration 1600 : Loss 3448.9858\n",
      "Iteration 1610 : Loss 3448.3205\n",
      "Iteration 1620 : Loss 3447.6563\n",
      "Iteration 1630 : Loss 3446.9933\n",
      "Iteration 1640 : Loss 3446.3314\n",
      "Iteration 1650 : Loss 3445.6707\n",
      "Iteration 1660 : Loss 3445.0111\n",
      "Iteration 1670 : Loss 3444.3527\n",
      "Iteration 1680 : Loss 3443.6954\n",
      "Iteration 1690 : Loss 3443.0392\n",
      "Iteration 1700 : Loss 3442.3842\n",
      "Iteration 1710 : Loss 3441.7303\n",
      "Iteration 1720 : Loss 3441.0775\n",
      "Iteration 1730 : Loss 3440.4258\n",
      "Iteration 1740 : Loss 3439.7753\n",
      "Iteration 1750 : Loss 3439.1258\n",
      "Iteration 1760 : Loss 3438.4775\n",
      "Iteration 1770 : Loss 3437.8303\n",
      "Iteration 1780 : Loss 3437.1842\n",
      "Iteration 1790 : Loss 3436.5392\n",
      "Iteration 1800 : Loss 3435.8954\n",
      "Iteration 1810 : Loss 3435.2526\n",
      "Iteration 1820 : Loss 3434.6109\n",
      "Iteration 1830 : Loss 3433.9703\n",
      "Iteration 1840 : Loss 3433.3308\n",
      "Iteration 1850 : Loss 3432.6924\n",
      "Iteration 1860 : Loss 3432.0551\n",
      "Iteration 1870 : Loss 3431.4188\n",
      "Iteration 1880 : Loss 3430.7837\n",
      "Iteration 1890 : Loss 3430.1496\n",
      "Iteration 1900 : Loss 3429.5166\n",
      "Iteration 1910 : Loss 3428.8847\n",
      "Iteration 1920 : Loss 3428.2538\n",
      "Iteration 1930 : Loss 3427.6240\n",
      "Iteration 1940 : Loss 3426.9953\n",
      "Iteration 1950 : Loss 3426.3676\n",
      "Iteration 1960 : Loss 3425.7410\n",
      "Iteration 1970 : Loss 3425.1155\n",
      "Iteration 1980 : Loss 3424.4910\n",
      "Iteration 1990 : Loss 3423.8675\n",
      "Iteration 2000 : Loss 3423.2451\n",
      "Iteration 2010 : Loss 3422.6238\n",
      "Iteration 2020 : Loss 3422.0035\n",
      "Iteration 2030 : Loss 3421.3842\n",
      "Iteration 2040 : Loss 3420.7660\n",
      "Iteration 2050 : Loss 3420.1488\n",
      "Iteration 2060 : Loss 3419.5327\n",
      "Iteration 2070 : Loss 3418.9176\n",
      "Iteration 2080 : Loss 3418.3035\n",
      "Iteration 2090 : Loss 3417.6904\n",
      "Iteration 2100 : Loss 3417.0784\n",
      "Iteration 2110 : Loss 3416.4674\n",
      "Iteration 2120 : Loss 3415.8574\n",
      "Iteration 2130 : Loss 3415.2484\n",
      "Iteration 2140 : Loss 3414.6404\n",
      "Iteration 2150 : Loss 3414.0335\n",
      "Iteration 2160 : Loss 3413.4275\n",
      "Iteration 2170 : Loss 3412.8226\n",
      "Iteration 2180 : Loss 3412.2186\n",
      "Iteration 2190 : Loss 3411.6157\n",
      "Iteration 2200 : Loss 3411.0138\n",
      "Iteration 2210 : Loss 3410.4128\n",
      "Iteration 2220 : Loss 3409.8129\n",
      "Iteration 2230 : Loss 3409.2139\n",
      "Iteration 2240 : Loss 3408.6160\n",
      "Iteration 2250 : Loss 3408.0190\n",
      "Iteration 2260 : Loss 3407.4230\n",
      "Iteration 2270 : Loss 3406.8280\n",
      "Iteration 2280 : Loss 3406.2340\n",
      "Iteration 2290 : Loss 3405.6409\n",
      "Iteration 2300 : Loss 3405.0489\n",
      "Iteration 2310 : Loss 3404.4578\n",
      "Iteration 2320 : Loss 3403.8676\n",
      "Iteration 2330 : Loss 3403.2785\n",
      "Iteration 2340 : Loss 3402.6903\n",
      "Iteration 2350 : Loss 3402.1030\n",
      "Iteration 2360 : Loss 3401.5168\n",
      "Iteration 2370 : Loss 3400.9315\n",
      "Iteration 2380 : Loss 3400.3471\n",
      "Iteration 2390 : Loss 3399.7637\n",
      "Iteration 2400 : Loss 3399.1813\n",
      "Iteration 2410 : Loss 3398.5998\n",
      "Iteration 2420 : Loss 3398.0192\n",
      "Iteration 2430 : Loss 3397.4396\n",
      "Iteration 2440 : Loss 3396.8609\n",
      "Iteration 2450 : Loss 3396.2832\n",
      "Iteration 2460 : Loss 3395.7064\n",
      "Iteration 2470 : Loss 3395.1306\n",
      "Iteration 2480 : Loss 3394.5557\n",
      "Iteration 2490 : Loss 3393.9817\n",
      "Iteration 2500 : Loss 3393.4086\n",
      "Iteration 2510 : Loss 3392.8365\n",
      "Iteration 2520 : Loss 3392.2653\n",
      "Iteration 2530 : Loss 3391.6950\n",
      "Iteration 2540 : Loss 3391.1257\n",
      "Iteration 2550 : Loss 3390.5572\n",
      "Iteration 2560 : Loss 3389.9897\n",
      "Iteration 2570 : Loss 3389.4231\n",
      "Iteration 2580 : Loss 3388.8574\n",
      "Iteration 2590 : Loss 3388.2926\n",
      "Iteration 2600 : Loss 3387.7288\n",
      "Iteration 2610 : Loss 3387.1658\n",
      "Iteration 2620 : Loss 3386.6037\n",
      "Iteration 2630 : Loss 3386.0425\n",
      "Iteration 2640 : Loss 3385.4823\n",
      "Iteration 2650 : Loss 3384.9229\n",
      "Iteration 2660 : Loss 3384.3644\n",
      "Iteration 2670 : Loss 3383.8068\n",
      "Iteration 2680 : Loss 3383.2502\n",
      "Iteration 2690 : Loss 3382.6943\n",
      "Iteration 2700 : Loss 3382.1394\n",
      "Iteration 2710 : Loss 3381.5854\n",
      "Iteration 2720 : Loss 3381.0322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2730 : Loss 3380.4800\n",
      "Iteration 2740 : Loss 3379.9286\n",
      "Iteration 2750 : Loss 3379.3781\n",
      "Iteration 2760 : Loss 3378.8284\n",
      "Iteration 2770 : Loss 3378.2796\n",
      "Iteration 2780 : Loss 3377.7317\n",
      "Iteration 2790 : Loss 3377.1847\n",
      "Iteration 2800 : Loss 3376.6385\n",
      "Iteration 2810 : Loss 3376.0932\n",
      "Iteration 2820 : Loss 3375.5488\n",
      "Iteration 2830 : Loss 3375.0052\n",
      "Iteration 2840 : Loss 3374.4625\n",
      "Iteration 2850 : Loss 3373.9206\n",
      "Iteration 2860 : Loss 3373.3796\n",
      "Iteration 2870 : Loss 3372.8395\n",
      "Iteration 2880 : Loss 3372.3001\n",
      "Iteration 2890 : Loss 3371.7617\n",
      "Iteration 2900 : Loss 3371.2241\n",
      "Iteration 2910 : Loss 3370.6873\n",
      "Iteration 2920 : Loss 3370.1514\n",
      "Iteration 2930 : Loss 3369.6163\n",
      "Iteration 2940 : Loss 3369.0820\n",
      "Iteration 2950 : Loss 3368.5486\n",
      "Iteration 2960 : Loss 3368.0160\n",
      "Iteration 2970 : Loss 3367.4843\n",
      "Iteration 2980 : Loss 3366.9533\n",
      "Iteration 2990 : Loss 3366.4233\n",
      "Iteration 3000 : Loss 3365.8940\n",
      "Iteration 3010 : Loss 3365.3655\n",
      "Iteration 3020 : Loss 3364.8379\n",
      "Iteration 3030 : Loss 3364.3111\n",
      "Iteration 3040 : Loss 3363.7852\n",
      "Iteration 3050 : Loss 3363.2600\n",
      "Iteration 3060 : Loss 3362.7357\n",
      "Iteration 3070 : Loss 3362.2121\n",
      "Iteration 3080 : Loss 3361.6894\n",
      "Iteration 3090 : Loss 3361.1675\n",
      "Iteration 3100 : Loss 3360.6464\n",
      "Iteration 3110 : Loss 3360.1261\n",
      "Iteration 3120 : Loss 3359.6066\n",
      "Iteration 3130 : Loss 3359.0879\n",
      "Iteration 3140 : Loss 3358.5700\n",
      "Iteration 3150 : Loss 3358.0529\n",
      "Iteration 3160 : Loss 3357.5366\n",
      "Iteration 3170 : Loss 3357.0211\n",
      "Iteration 3180 : Loss 3356.5064\n",
      "Iteration 3190 : Loss 3355.9925\n",
      "Iteration 3200 : Loss 3355.4794\n",
      "Iteration 3210 : Loss 3354.9671\n",
      "Iteration 3220 : Loss 3354.4555\n",
      "Iteration 3230 : Loss 3353.9448\n",
      "Iteration 3240 : Loss 3353.4348\n",
      "Iteration 3250 : Loss 3352.9256\n",
      "Iteration 3260 : Loss 3352.4171\n",
      "Iteration 3270 : Loss 3351.9095\n",
      "Iteration 3280 : Loss 3351.4026\n",
      "Iteration 3290 : Loss 3350.8965\n",
      "Iteration 3300 : Loss 3350.3912\n",
      "Iteration 3310 : Loss 3349.8867\n",
      "Iteration 3320 : Loss 3349.3829\n",
      "Iteration 3330 : Loss 3348.8798\n",
      "Iteration 3340 : Loss 3348.3776\n",
      "Iteration 3350 : Loss 3347.8761\n",
      "Iteration 3360 : Loss 3347.3754\n",
      "Iteration 3370 : Loss 3346.8754\n",
      "Iteration 3380 : Loss 3346.3762\n",
      "Iteration 3390 : Loss 3345.8777\n",
      "Iteration 3400 : Loss 3345.3800\n",
      "Iteration 3410 : Loss 3344.8831\n",
      "Iteration 3420 : Loss 3344.3869\n",
      "Iteration 3430 : Loss 3343.8915\n",
      "Iteration 3440 : Loss 3343.3968\n",
      "Iteration 3450 : Loss 3342.9028\n",
      "Iteration 3460 : Loss 3342.4096\n",
      "Iteration 3470 : Loss 3341.9171\n",
      "Iteration 3480 : Loss 3341.4254\n",
      "Iteration 3490 : Loss 3340.9344\n",
      "Iteration 3500 : Loss 3340.4442\n",
      "Iteration 3510 : Loss 3339.9547\n",
      "Iteration 3520 : Loss 3339.4659\n",
      "Iteration 3530 : Loss 3338.9779\n",
      "Iteration 3540 : Loss 3338.4906\n",
      "Iteration 3550 : Loss 3338.0040\n",
      "Iteration 3560 : Loss 3337.5181\n",
      "Iteration 3570 : Loss 3337.0330\n",
      "Iteration 3580 : Loss 3336.5486\n",
      "Iteration 3590 : Loss 3336.0650\n",
      "Iteration 3600 : Loss 3335.5820\n",
      "Iteration 3610 : Loss 3335.0998\n",
      "Iteration 3620 : Loss 3334.6183\n",
      "Iteration 3630 : Loss 3334.1375\n",
      "Iteration 3640 : Loss 3333.6574\n",
      "Iteration 3650 : Loss 3333.1780\n",
      "Iteration 3660 : Loss 3332.6994\n",
      "Iteration 3670 : Loss 3332.2214\n",
      "Iteration 3680 : Loss 3331.7442\n",
      "Iteration 3690 : Loss 3331.2677\n",
      "Iteration 3700 : Loss 3330.7919\n",
      "Iteration 3710 : Loss 3330.3168\n",
      "Iteration 3720 : Loss 3329.8423\n",
      "Iteration 3730 : Loss 3329.3686\n",
      "Iteration 3740 : Loss 3328.8956\n",
      "Iteration 3750 : Loss 3328.4233\n",
      "Iteration 3760 : Loss 3327.9517\n",
      "Iteration 3770 : Loss 3327.4808\n",
      "Iteration 3780 : Loss 3327.0106\n",
      "Iteration 3790 : Loss 3326.5410\n",
      "Iteration 3800 : Loss 3326.0722\n",
      "Iteration 3810 : Loss 3325.6041\n",
      "Iteration 3820 : Loss 3325.1366\n",
      "Iteration 3830 : Loss 3324.6698\n",
      "Iteration 3840 : Loss 3324.2037\n",
      "Iteration 3850 : Loss 3323.7383\n",
      "Iteration 3860 : Loss 3323.2736\n",
      "Iteration 3870 : Loss 3322.8096\n",
      "Iteration 3880 : Loss 3322.3462\n",
      "Iteration 3890 : Loss 3321.8835\n",
      "Iteration 3900 : Loss 3321.4215\n",
      "Iteration 3910 : Loss 3320.9602\n",
      "Iteration 3920 : Loss 3320.4995\n",
      "Iteration 3930 : Loss 3320.0395\n",
      "Iteration 3940 : Loss 3319.5802\n",
      "Iteration 3950 : Loss 3319.1216\n",
      "Iteration 3960 : Loss 3318.6636\n",
      "Iteration 3970 : Loss 3318.2063\n",
      "Iteration 3980 : Loss 3317.7496\n",
      "Iteration 3990 : Loss 3317.2936\n",
      "Iteration 4000 : Loss 3316.8383\n",
      "Iteration 4010 : Loss 3316.3836\n",
      "Iteration 4020 : Loss 3315.9296\n",
      "Iteration 4030 : Loss 3315.4763\n",
      "Iteration 4040 : Loss 3315.0236\n",
      "Iteration 4050 : Loss 3314.5715\n",
      "Iteration 4060 : Loss 3314.1202\n",
      "Iteration 4070 : Loss 3313.6694\n",
      "Iteration 4080 : Loss 3313.2193\n",
      "Iteration 4090 : Loss 3312.7699\n",
      "Iteration 4100 : Loss 3312.3211\n",
      "Iteration 4110 : Loss 3311.8730\n",
      "Iteration 4120 : Loss 3311.4255\n",
      "Iteration 4130 : Loss 3310.9786\n",
      "Iteration 4140 : Loss 3310.5324\n",
      "Iteration 4150 : Loss 3310.0869\n",
      "Iteration 4160 : Loss 3309.6419\n",
      "Iteration 4170 : Loss 3309.1977\n",
      "Iteration 4180 : Loss 3308.7540\n",
      "Iteration 4190 : Loss 3308.3110\n",
      "Iteration 4200 : Loss 3307.8686\n",
      "Iteration 4210 : Loss 3307.4269\n",
      "Iteration 4220 : Loss 3306.9858\n",
      "Iteration 4230 : Loss 3306.5453\n",
      "Iteration 4240 : Loss 3306.1054\n",
      "Iteration 4250 : Loss 3305.6662\n",
      "Iteration 4260 : Loss 3305.2276\n",
      "Iteration 4270 : Loss 3304.7896\n",
      "Iteration 4280 : Loss 3304.3523\n",
      "Iteration 4290 : Loss 3303.9155\n",
      "Iteration 4300 : Loss 3303.4794\n",
      "Iteration 4310 : Loss 3303.0439\n",
      "Iteration 4320 : Loss 3302.6091\n",
      "Iteration 4330 : Loss 3302.1748\n",
      "Iteration 4340 : Loss 3301.7412\n",
      "Iteration 4350 : Loss 3301.3081\n",
      "Iteration 4360 : Loss 3300.8757\n",
      "Iteration 4370 : Loss 3300.4439\n",
      "Iteration 4380 : Loss 3300.0127\n",
      "Iteration 4390 : Loss 3299.5822\n",
      "Iteration 4400 : Loss 3299.1522\n",
      "Iteration 4410 : Loss 3298.7228\n",
      "Iteration 4420 : Loss 3298.2941\n",
      "Iteration 4430 : Loss 3297.8659\n",
      "Iteration 4440 : Loss 3297.4384\n",
      "Iteration 4450 : Loss 3297.0114\n",
      "Iteration 4460 : Loss 3296.5851\n",
      "Iteration 4470 : Loss 3296.1593\n",
      "Iteration 4480 : Loss 3295.7342\n",
      "Iteration 4490 : Loss 3295.3096\n",
      "Iteration 4500 : Loss 3294.8857\n",
      "Iteration 4510 : Loss 3294.4623\n",
      "Iteration 4520 : Loss 3294.0396\n",
      "Iteration 4530 : Loss 3293.6174\n",
      "Iteration 4540 : Loss 3293.1958\n",
      "Iteration 4550 : Loss 3292.7748\n",
      "Iteration 4560 : Loss 3292.3544\n",
      "Iteration 4570 : Loss 3291.9346\n",
      "Iteration 4580 : Loss 3291.5154\n",
      "Iteration 4590 : Loss 3291.0967\n",
      "Iteration 4600 : Loss 3290.6787\n",
      "Iteration 4610 : Loss 3290.2612\n",
      "Iteration 4620 : Loss 3289.8443\n",
      "Iteration 4630 : Loss 3289.4280\n",
      "Iteration 4640 : Loss 3289.0122\n",
      "Iteration 4650 : Loss 3288.5971\n",
      "Iteration 4660 : Loss 3288.1825\n",
      "Iteration 4670 : Loss 3287.7685\n",
      "Iteration 4680 : Loss 3287.3550\n",
      "Iteration 4690 : Loss 3286.9422\n",
      "Iteration 4700 : Loss 3286.5299\n",
      "Iteration 4710 : Loss 3286.1182\n",
      "Iteration 4720 : Loss 3285.7070\n",
      "Iteration 4730 : Loss 3285.2964\n",
      "Iteration 4740 : Loss 3284.8864\n",
      "Iteration 4750 : Loss 3284.4770\n",
      "Iteration 4760 : Loss 3284.0681\n",
      "Iteration 4770 : Loss 3283.6598\n",
      "Iteration 4780 : Loss 3283.2520\n",
      "Iteration 4790 : Loss 3282.8448\n",
      "Iteration 4800 : Loss 3282.4382\n",
      "Iteration 4810 : Loss 3282.0321\n",
      "Iteration 4820 : Loss 3281.6266\n",
      "Iteration 4830 : Loss 3281.2216\n",
      "Iteration 4840 : Loss 3280.8172\n",
      "Iteration 4850 : Loss 3280.4134\n",
      "Iteration 4860 : Loss 3280.0101\n",
      "Iteration 4870 : Loss 3279.6074\n",
      "Iteration 4880 : Loss 3279.2052\n",
      "Iteration 4890 : Loss 3278.8035\n",
      "Iteration 4900 : Loss 3278.4024\n",
      "Iteration 4910 : Loss 3278.0019\n",
      "Iteration 4920 : Loss 3277.6019\n",
      "Iteration 4930 : Loss 3277.2025\n",
      "Iteration 4940 : Loss 3276.8036\n",
      "Iteration 4950 : Loss 3276.4052\n",
      "Iteration 4960 : Loss 3276.0074\n",
      "Iteration 4970 : Loss 3275.6101\n",
      "Iteration 4980 : Loss 3275.2134\n",
      "Iteration 4990 : Loss 3274.8172\n",
      "Iteration 5000 : Loss 3274.4215\n",
      "Iteration 5010 : Loss 3274.0264\n",
      "Iteration 5020 : Loss 3273.6318\n",
      "Iteration 5030 : Loss 3273.2378\n",
      "Iteration 5040 : Loss 3272.8443\n",
      "Iteration 5050 : Loss 3272.4513\n",
      "Iteration 5060 : Loss 3272.0589\n",
      "Iteration 5070 : Loss 3271.6669\n",
      "Iteration 5080 : Loss 3271.2756\n",
      "Iteration 5090 : Loss 3270.8847\n",
      "Iteration 5100 : Loss 3270.4944\n",
      "Iteration 5110 : Loss 3270.1046\n",
      "Iteration 5120 : Loss 3269.7153\n",
      "Iteration 5130 : Loss 3269.3266\n",
      "Iteration 5140 : Loss 3268.9383\n",
      "Iteration 5150 : Loss 3268.5506\n",
      "Iteration 5160 : Loss 3268.1635\n",
      "Iteration 5170 : Loss 3267.7768\n",
      "Iteration 5180 : Loss 3267.3907\n",
      "Iteration 5190 : Loss 3267.0050\n",
      "Iteration 5200 : Loss 3266.6199\n",
      "Iteration 5210 : Loss 3266.2353\n",
      "Iteration 5220 : Loss 3265.8513\n",
      "Iteration 5230 : Loss 3265.4677\n",
      "Iteration 5240 : Loss 3265.0847\n",
      "Iteration 5250 : Loss 3264.7022\n",
      "Iteration 5260 : Loss 3264.3201\n",
      "Iteration 5270 : Loss 3263.9386\n",
      "Iteration 5280 : Loss 3263.5576\n",
      "Iteration 5290 : Loss 3263.1771\n",
      "Iteration 5300 : Loss 3262.7972\n",
      "Iteration 5310 : Loss 3262.4177\n",
      "Iteration 5320 : Loss 3262.0387\n",
      "Iteration 5330 : Loss 3261.6603\n",
      "Iteration 5340 : Loss 3261.2823\n",
      "Iteration 5350 : Loss 3260.9049\n",
      "Iteration 5360 : Loss 3260.5279\n",
      "Iteration 5370 : Loss 3260.1514\n",
      "Iteration 5380 : Loss 3259.7755\n",
      "Iteration 5390 : Loss 3259.4000\n",
      "Iteration 5400 : Loss 3259.0251\n",
      "Iteration 5410 : Loss 3258.6506\n",
      "Iteration 5420 : Loss 3258.2767\n",
      "Iteration 5430 : Loss 3257.9032\n",
      "Iteration 5440 : Loss 3257.5302\n",
      "Iteration 5450 : Loss 3257.1578\n",
      "Iteration 5460 : Loss 3256.7858\n",
      "Iteration 5470 : Loss 3256.4143\n",
      "Iteration 5480 : Loss 3256.0433\n",
      "Iteration 5490 : Loss 3255.6728\n",
      "Iteration 5500 : Loss 3255.3027\n",
      "Iteration 5510 : Loss 3254.9332\n",
      "Iteration 5520 : Loss 3254.5641\n",
      "Iteration 5530 : Loss 3254.1956\n",
      "Iteration 5540 : Loss 3253.8275\n",
      "Iteration 5550 : Loss 3253.4599\n",
      "Iteration 5560 : Loss 3253.0928\n",
      "Iteration 5570 : Loss 3252.7262\n",
      "Iteration 5580 : Loss 3252.3600\n",
      "Iteration 5590 : Loss 3251.9943\n",
      "Iteration 5600 : Loss 3251.6291\n",
      "Iteration 5610 : Loss 3251.2644\n",
      "Iteration 5620 : Loss 3250.9002\n",
      "Iteration 5630 : Loss 3250.5364\n",
      "Iteration 5640 : Loss 3250.1732\n",
      "Iteration 5650 : Loss 3249.8104\n",
      "Iteration 5660 : Loss 3249.4480\n",
      "Iteration 5670 : Loss 3249.0862\n",
      "Iteration 5680 : Loss 3248.7248\n",
      "Iteration 5690 : Loss 3248.3639\n",
      "Iteration 5700 : Loss 3248.0034\n",
      "Iteration 5710 : Loss 3247.6434\n",
      "Iteration 5720 : Loss 3247.2839\n",
      "Iteration 5730 : Loss 3246.9249\n",
      "Iteration 5740 : Loss 3246.5663\n",
      "Iteration 5750 : Loss 3246.2082\n",
      "Iteration 5760 : Loss 3245.8506\n",
      "Iteration 5770 : Loss 3245.4934\n",
      "Iteration 5780 : Loss 3245.1367\n",
      "Iteration 5790 : Loss 3244.7805\n",
      "Iteration 5800 : Loss 3244.4247\n",
      "Iteration 5810 : Loss 3244.0693\n",
      "Iteration 5820 : Loss 3243.7145\n",
      "Iteration 5830 : Loss 3243.3601\n",
      "Iteration 5840 : Loss 3243.0061\n",
      "Iteration 5850 : Loss 3242.6526\n",
      "Iteration 5860 : Loss 3242.2996\n",
      "Iteration 5870 : Loss 3241.9470\n",
      "Iteration 5880 : Loss 3241.5949\n",
      "Iteration 5890 : Loss 3241.2432\n",
      "Iteration 5900 : Loss 3240.8920\n",
      "Iteration 5910 : Loss 3240.5412\n",
      "Iteration 5920 : Loss 3240.1909\n",
      "Iteration 5930 : Loss 3239.8411\n",
      "Iteration 5940 : Loss 3239.4917\n",
      "Iteration 5950 : Loss 3239.1427\n",
      "Iteration 5960 : Loss 3238.7942\n",
      "Iteration 5970 : Loss 3238.4461\n",
      "Iteration 5980 : Loss 3238.0985\n",
      "Iteration 5990 : Loss 3237.7513\n",
      "Iteration 6000 : Loss 3237.4046\n",
      "Iteration 6010 : Loss 3237.0583\n",
      "Iteration 6020 : Loss 3236.7125\n",
      "Iteration 6030 : Loss 3236.3671\n",
      "Iteration 6040 : Loss 3236.0221\n",
      "Iteration 6050 : Loss 3235.6776\n",
      "Iteration 6060 : Loss 3235.3335\n",
      "Iteration 6070 : Loss 3234.9899\n",
      "Iteration 6080 : Loss 3234.6467\n",
      "Iteration 6090 : Loss 3234.3040\n",
      "Iteration 6100 : Loss 3233.9616\n",
      "Iteration 6110 : Loss 3233.6197\n",
      "Iteration 6120 : Loss 3233.2783\n",
      "Iteration 6130 : Loss 3232.9373\n",
      "Iteration 6140 : Loss 3232.5967\n",
      "Iteration 6150 : Loss 3232.2566\n",
      "Iteration 6160 : Loss 3231.9168\n",
      "Iteration 6170 : Loss 3231.5775\n",
      "Iteration 6180 : Loss 3231.2387\n",
      "Iteration 6190 : Loss 3230.9003\n",
      "Iteration 6200 : Loss 3230.5623\n",
      "Iteration 6210 : Loss 3230.2247\n",
      "Iteration 6220 : Loss 3229.8876\n",
      "Iteration 6230 : Loss 3229.5509\n",
      "Iteration 6240 : Loss 3229.2146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6250 : Loss 3228.8787\n",
      "Iteration 6260 : Loss 3228.5433\n",
      "Iteration 6270 : Loss 3228.2083\n",
      "Iteration 6280 : Loss 3227.8737\n",
      "Iteration 6290 : Loss 3227.5395\n",
      "Iteration 6300 : Loss 3227.2058\n",
      "Iteration 6310 : Loss 3226.8724\n",
      "Iteration 6320 : Loss 3226.5395\n",
      "Iteration 6330 : Loss 3226.2070\n",
      "Iteration 6340 : Loss 3225.8750\n",
      "Iteration 6350 : Loss 3225.5433\n",
      "Iteration 6360 : Loss 3225.2121\n",
      "Iteration 6370 : Loss 3224.8813\n",
      "Iteration 6380 : Loss 3224.5509\n",
      "Iteration 6390 : Loss 3224.2209\n",
      "Iteration 6400 : Loss 3223.8913\n",
      "Iteration 6410 : Loss 3223.5622\n",
      "Iteration 6420 : Loss 3223.2334\n",
      "Iteration 6430 : Loss 3222.9051\n",
      "Iteration 6440 : Loss 3222.5772\n",
      "Iteration 6450 : Loss 3222.2497\n",
      "Iteration 6460 : Loss 3221.9226\n",
      "Iteration 6470 : Loss 3221.5959\n",
      "Iteration 6480 : Loss 3221.2696\n",
      "Iteration 6490 : Loss 3220.9437\n",
      "Iteration 6500 : Loss 3220.6182\n",
      "Iteration 6510 : Loss 3220.2932\n",
      "Iteration 6520 : Loss 3219.9685\n",
      "Iteration 6530 : Loss 3219.6442\n",
      "Iteration 6540 : Loss 3219.3204\n",
      "Iteration 6550 : Loss 3218.9969\n",
      "Iteration 6560 : Loss 3218.6739\n",
      "Iteration 6570 : Loss 3218.3513\n",
      "Iteration 6580 : Loss 3218.0290\n",
      "Iteration 6590 : Loss 3217.7072\n",
      "Iteration 6600 : Loss 3217.3857\n",
      "Iteration 6610 : Loss 3217.0647\n",
      "Iteration 6620 : Loss 3216.7440\n",
      "Iteration 6630 : Loss 3216.4238\n",
      "Iteration 6640 : Loss 3216.1039\n",
      "Iteration 6650 : Loss 3215.7845\n",
      "Iteration 6660 : Loss 3215.4654\n",
      "Iteration 6670 : Loss 3215.1468\n",
      "Iteration 6680 : Loss 3214.8285\n",
      "Iteration 6690 : Loss 3214.5106\n",
      "Iteration 6700 : Loss 3214.1931\n",
      "Iteration 6710 : Loss 3213.8760\n",
      "Iteration 6720 : Loss 3213.5593\n",
      "Iteration 6730 : Loss 3213.2430\n",
      "Iteration 6740 : Loss 3212.9271\n",
      "Iteration 6750 : Loss 3212.6116\n",
      "Iteration 6760 : Loss 3212.2964\n",
      "Iteration 6770 : Loss 3211.9817\n",
      "Iteration 6780 : Loss 3211.6673\n",
      "Iteration 6790 : Loss 3211.3533\n",
      "Iteration 6800 : Loss 3211.0398\n",
      "Iteration 6810 : Loss 3210.7265\n",
      "Iteration 6820 : Loss 3210.4137\n",
      "Iteration 6830 : Loss 3210.1013\n",
      "Iteration 6840 : Loss 3209.7892\n",
      "Iteration 6850 : Loss 3209.4776\n",
      "Iteration 6860 : Loss 3209.1663\n",
      "Iteration 6870 : Loss 3208.8554\n",
      "Iteration 6880 : Loss 3208.5448\n",
      "Iteration 6890 : Loss 3208.2347\n",
      "Iteration 6900 : Loss 3207.9249\n",
      "Iteration 6910 : Loss 3207.6155\n",
      "Iteration 6920 : Loss 3207.3065\n",
      "Iteration 6930 : Loss 3206.9979\n",
      "Iteration 6940 : Loss 3206.6896\n",
      "Iteration 6950 : Loss 3206.3817\n",
      "Iteration 6960 : Loss 3206.0742\n",
      "Iteration 6970 : Loss 3205.7671\n",
      "Iteration 6980 : Loss 3205.4603\n",
      "Iteration 6990 : Loss 3205.1540\n",
      "Iteration 7000 : Loss 3204.8480\n",
      "Iteration 7010 : Loss 3204.5423\n",
      "Iteration 7020 : Loss 3204.2370\n",
      "Iteration 7030 : Loss 3203.9322\n",
      "Iteration 7040 : Loss 3203.6276\n",
      "Iteration 7050 : Loss 3203.3235\n",
      "Iteration 7060 : Loss 3203.0197\n",
      "Iteration 7070 : Loss 3202.7163\n",
      "Iteration 7080 : Loss 3202.4132\n",
      "Iteration 7090 : Loss 3202.1106\n",
      "Iteration 7100 : Loss 3201.8082\n",
      "Iteration 7110 : Loss 3201.5063\n",
      "Iteration 7120 : Loss 3201.2047\n",
      "Iteration 7130 : Loss 3200.9035\n",
      "Iteration 7140 : Loss 3200.6026\n",
      "Iteration 7150 : Loss 3200.3022\n",
      "Iteration 7160 : Loss 3200.0020\n",
      "Iteration 7170 : Loss 3199.7023\n",
      "Iteration 7180 : Loss 3199.4029\n",
      "Iteration 7190 : Loss 3199.1038\n",
      "Iteration 7200 : Loss 3198.8051\n",
      "Iteration 7210 : Loss 3198.5068\n",
      "Iteration 7220 : Loss 3198.2089\n",
      "Iteration 7230 : Loss 3197.9113\n",
      "Iteration 7240 : Loss 3197.6140\n",
      "Iteration 7250 : Loss 3197.3171\n",
      "Iteration 7260 : Loss 3197.0206\n",
      "Iteration 7270 : Loss 3196.7244\n",
      "Iteration 7280 : Loss 3196.4286\n",
      "Iteration 7290 : Loss 3196.1332\n",
      "Iteration 7300 : Loss 3195.8381\n",
      "Iteration 7310 : Loss 3195.5433\n",
      "Iteration 7320 : Loss 3195.2489\n",
      "Iteration 7330 : Loss 3194.9549\n",
      "Iteration 7340 : Loss 3194.6612\n",
      "Iteration 7350 : Loss 3194.3678\n",
      "Iteration 7360 : Loss 3194.0749\n",
      "Iteration 7370 : Loss 3193.7822\n",
      "Iteration 7380 : Loss 3193.4899\n",
      "Iteration 7390 : Loss 3193.1980\n",
      "Iteration 7400 : Loss 3192.9064\n",
      "Iteration 7410 : Loss 3192.6152\n",
      "Iteration 7420 : Loss 3192.3243\n",
      "Iteration 7430 : Loss 3192.0337\n",
      "Iteration 7440 : Loss 3191.7435\n",
      "Iteration 7450 : Loss 3191.4537\n",
      "Iteration 7460 : Loss 3191.1642\n",
      "Iteration 7470 : Loss 3190.8750\n",
      "Iteration 7480 : Loss 3190.5862\n",
      "Iteration 7490 : Loss 3190.2977\n",
      "Iteration 7500 : Loss 3190.0096\n",
      "Iteration 7510 : Loss 3189.7218\n",
      "Iteration 7520 : Loss 3189.4343\n",
      "Iteration 7530 : Loss 3189.1472\n",
      "Iteration 7540 : Loss 3188.8605\n",
      "Iteration 7550 : Loss 3188.5740\n",
      "Iteration 7560 : Loss 3188.2880\n",
      "Iteration 7570 : Loss 3188.0022\n",
      "Iteration 7580 : Loss 3187.7168\n",
      "Iteration 7590 : Loss 3187.4317\n",
      "Iteration 7600 : Loss 3187.1470\n",
      "Iteration 7610 : Loss 3186.8626\n",
      "Iteration 7620 : Loss 3186.5786\n",
      "Iteration 7630 : Loss 3186.2948\n",
      "Iteration 7640 : Loss 3186.0115\n",
      "Iteration 7650 : Loss 3185.7284\n",
      "Iteration 7660 : Loss 3185.4457\n",
      "Iteration 7670 : Loss 3185.1633\n",
      "Iteration 7680 : Loss 3184.8813\n",
      "Iteration 7690 : Loss 3184.5995\n",
      "Iteration 7700 : Loss 3184.3182\n",
      "Iteration 7710 : Loss 3184.0371\n",
      "Iteration 7720 : Loss 3183.7564\n",
      "Iteration 7730 : Loss 3183.4760\n",
      "Iteration 7740 : Loss 3183.1959\n",
      "Iteration 7750 : Loss 3182.9162\n",
      "Iteration 7760 : Loss 3182.6368\n",
      "Iteration 7770 : Loss 3182.3577\n",
      "Iteration 7780 : Loss 3182.0790\n",
      "Iteration 7790 : Loss 3181.8006\n",
      "Iteration 7800 : Loss 3181.5225\n",
      "Iteration 7810 : Loss 3181.2447\n",
      "Iteration 7820 : Loss 3180.9673\n",
      "Iteration 7830 : Loss 3180.6902\n",
      "Iteration 7840 : Loss 3180.4134\n",
      "Iteration 7850 : Loss 3180.1369\n",
      "Iteration 7860 : Loss 3179.8608\n",
      "Iteration 7870 : Loss 3179.5850\n",
      "Iteration 7880 : Loss 3179.3095\n",
      "Iteration 7890 : Loss 3179.0343\n",
      "Iteration 7900 : Loss 3178.7595\n",
      "Iteration 7910 : Loss 3178.4849\n",
      "Iteration 7920 : Loss 3178.2107\n",
      "Iteration 7930 : Loss 3177.9369\n",
      "Iteration 7940 : Loss 3177.6633\n",
      "Iteration 7950 : Loss 3177.3900\n",
      "Iteration 7960 : Loss 3177.1171\n",
      "Iteration 7970 : Loss 3176.8445\n",
      "Iteration 7980 : Loss 3176.5722\n",
      "Iteration 7990 : Loss 3176.3002\n",
      "Iteration 8000 : Loss 3176.0286\n",
      "Iteration 8010 : Loss 3175.7572\n",
      "Iteration 8020 : Loss 3175.4862\n",
      "Iteration 8030 : Loss 3175.2155\n",
      "Iteration 8040 : Loss 3174.9451\n",
      "Iteration 8050 : Loss 3174.6750\n",
      "Iteration 8060 : Loss 3174.4053\n",
      "Iteration 8070 : Loss 3174.1358\n",
      "Iteration 8080 : Loss 3173.8667\n",
      "Iteration 8090 : Loss 3173.5978\n",
      "Iteration 8100 : Loss 3173.3293\n",
      "Iteration 8110 : Loss 3173.0611\n",
      "Iteration 8120 : Loss 3172.7932\n",
      "Iteration 8130 : Loss 3172.5256\n",
      "Iteration 8140 : Loss 3172.2584\n",
      "Iteration 8150 : Loss 3171.9914\n",
      "Iteration 8160 : Loss 3171.7247\n",
      "Iteration 8170 : Loss 3171.4584\n",
      "Iteration 8180 : Loss 3171.1924\n",
      "Iteration 8190 : Loss 3170.9266\n",
      "Iteration 8200 : Loss 3170.6612\n",
      "Iteration 8210 : Loss 3170.3961\n",
      "Iteration 8220 : Loss 3170.1313\n",
      "Iteration 8230 : Loss 3169.8668\n",
      "Iteration 8240 : Loss 3169.6026\n",
      "Iteration 8250 : Loss 3169.3387\n",
      "Iteration 8260 : Loss 3169.0751\n",
      "Iteration 8270 : Loss 3168.8118\n",
      "Iteration 8280 : Loss 3168.5488\n",
      "Iteration 8290 : Loss 3168.2861\n",
      "Iteration 8300 : Loss 3168.0237\n",
      "Iteration 8310 : Loss 3167.7616\n",
      "Iteration 8320 : Loss 3167.4999\n",
      "Iteration 8330 : Loss 3167.2384\n",
      "Iteration 8340 : Loss 3166.9772\n",
      "Iteration 8350 : Loss 3166.7163\n",
      "Iteration 8360 : Loss 3166.4557\n",
      "Iteration 8370 : Loss 3166.1955\n",
      "Iteration 8380 : Loss 3165.9355\n",
      "Iteration 8390 : Loss 3165.6758\n",
      "Iteration 8400 : Loss 3165.4164\n",
      "Iteration 8410 : Loss 3165.1573\n",
      "Iteration 8420 : Loss 3164.8985\n",
      "Iteration 8430 : Loss 3164.6401\n",
      "Iteration 8440 : Loss 3164.3819\n",
      "Iteration 8450 : Loss 3164.1239\n",
      "Iteration 8460 : Loss 3163.8663\n",
      "Iteration 8470 : Loss 3163.6090\n",
      "Iteration 8480 : Loss 3163.3520\n",
      "Iteration 8490 : Loss 3163.0953\n",
      "Iteration 8500 : Loss 3162.8389\n",
      "Iteration 8510 : Loss 3162.5827\n",
      "Iteration 8520 : Loss 3162.3269\n",
      "Iteration 8530 : Loss 3162.0713\n",
      "Iteration 8540 : Loss 3161.8160\n",
      "Iteration 8550 : Loss 3161.5611\n",
      "Iteration 8560 : Loss 3161.3064\n",
      "Iteration 8570 : Loss 3161.0520\n",
      "Iteration 8580 : Loss 3160.7979\n",
      "Iteration 8590 : Loss 3160.5441\n",
      "Iteration 8600 : Loss 3160.2905\n",
      "Iteration 8610 : Loss 3160.0373\n",
      "Iteration 8620 : Loss 3159.7844\n",
      "Iteration 8630 : Loss 3159.5317\n",
      "Iteration 8640 : Loss 3159.2793\n",
      "Iteration 8650 : Loss 3159.0272\n",
      "Iteration 8660 : Loss 3158.7754\n",
      "Iteration 8670 : Loss 3158.5239\n",
      "Iteration 8680 : Loss 3158.2727\n",
      "Iteration 8690 : Loss 3158.0217\n",
      "Iteration 8700 : Loss 3157.7711\n",
      "Iteration 8710 : Loss 3157.5207\n",
      "Iteration 8720 : Loss 3157.2706\n",
      "Iteration 8730 : Loss 3157.0208\n",
      "Iteration 8740 : Loss 3156.7713\n",
      "Iteration 8750 : Loss 3156.5220\n",
      "Iteration 8760 : Loss 3156.2731\n",
      "Iteration 8770 : Loss 3156.0244\n",
      "Iteration 8780 : Loss 3155.7760\n",
      "Iteration 8790 : Loss 3155.5279\n",
      "Iteration 8800 : Loss 3155.2800\n",
      "Iteration 8810 : Loss 3155.0324\n",
      "Iteration 8820 : Loss 3154.7852\n",
      "Iteration 8830 : Loss 3154.5382\n",
      "Iteration 8840 : Loss 3154.2914\n",
      "Iteration 8850 : Loss 3154.0450\n",
      "Iteration 8860 : Loss 3153.7988\n",
      "Iteration 8870 : Loss 3153.5529\n",
      "Iteration 8880 : Loss 3153.3073\n",
      "Iteration 8890 : Loss 3153.0620\n",
      "Iteration 8900 : Loss 3152.8169\n",
      "Iteration 8910 : Loss 3152.5721\n",
      "Iteration 8920 : Loss 3152.3276\n",
      "Iteration 8930 : Loss 3152.0834\n",
      "Iteration 8940 : Loss 3151.8394\n",
      "Iteration 8950 : Loss 3151.5957\n",
      "Iteration 8960 : Loss 3151.3523\n",
      "Iteration 8970 : Loss 3151.1092\n",
      "Iteration 8980 : Loss 3150.8663\n",
      "Iteration 8990 : Loss 3150.6237\n",
      "Iteration 9000 : Loss 3150.3814\n",
      "Iteration 9010 : Loss 3150.1393\n",
      "Iteration 9020 : Loss 3149.8976\n",
      "Iteration 9030 : Loss 3149.6561\n",
      "Iteration 9040 : Loss 3149.4148\n",
      "Iteration 9050 : Loss 3149.1738\n",
      "Iteration 9060 : Loss 3148.9331\n",
      "Iteration 9070 : Loss 3148.6927\n",
      "Iteration 9080 : Loss 3148.4526\n",
      "Iteration 9090 : Loss 3148.2127\n",
      "Iteration 9100 : Loss 3147.9730\n",
      "Iteration 9110 : Loss 3147.7337\n",
      "Iteration 9120 : Loss 3147.4946\n",
      "Iteration 9130 : Loss 3147.2558\n",
      "Iteration 9140 : Loss 3147.0172\n",
      "Iteration 9150 : Loss 3146.7789\n",
      "Iteration 9160 : Loss 3146.5409\n",
      "Iteration 9170 : Loss 3146.3031\n",
      "Iteration 9180 : Loss 3146.0656\n",
      "Iteration 9190 : Loss 3145.8284\n",
      "Iteration 9200 : Loss 3145.5914\n",
      "Iteration 9210 : Loss 3145.3547\n",
      "Iteration 9220 : Loss 3145.1183\n",
      "Iteration 9230 : Loss 3144.8821\n",
      "Iteration 9240 : Loss 3144.6462\n",
      "Iteration 9250 : Loss 3144.4106\n",
      "Iteration 9260 : Loss 3144.1752\n",
      "Iteration 9270 : Loss 3143.9400\n",
      "Iteration 9280 : Loss 3143.7052\n",
      "Iteration 9290 : Loss 3143.4706\n",
      "Iteration 9300 : Loss 3143.2362\n",
      "Iteration 9310 : Loss 3143.0021\n",
      "Iteration 9320 : Loss 3142.7683\n",
      "Iteration 9330 : Loss 3142.5347\n",
      "Iteration 9340 : Loss 3142.3014\n",
      "Iteration 9350 : Loss 3142.0684\n",
      "Iteration 9360 : Loss 3141.8356\n",
      "Iteration 9370 : Loss 3141.6031\n",
      "Iteration 9380 : Loss 3141.3708\n",
      "Iteration 9390 : Loss 3141.1388\n",
      "Iteration 9400 : Loss 3140.9070\n",
      "Iteration 9410 : Loss 3140.6755\n",
      "Iteration 9420 : Loss 3140.4443\n",
      "Iteration 9430 : Loss 3140.2133\n",
      "Iteration 9440 : Loss 3139.9825\n",
      "Iteration 9450 : Loss 3139.7520\n",
      "Iteration 9460 : Loss 3139.5218\n",
      "Iteration 9470 : Loss 3139.2918\n",
      "Iteration 9480 : Loss 3139.0621\n",
      "Iteration 9490 : Loss 3138.8326\n",
      "Iteration 9500 : Loss 3138.6034\n",
      "Iteration 9510 : Loss 3138.3745\n",
      "Iteration 9520 : Loss 3138.1458\n",
      "Iteration 9530 : Loss 3137.9173\n",
      "Iteration 9540 : Loss 3137.6891\n",
      "Iteration 9550 : Loss 3137.4611\n",
      "Iteration 9560 : Loss 3137.2334\n",
      "Iteration 9570 : Loss 3137.0060\n",
      "Iteration 9580 : Loss 3136.7788\n",
      "Iteration 9590 : Loss 3136.5518\n",
      "Iteration 9600 : Loss 3136.3251\n",
      "Iteration 9610 : Loss 3136.0987\n",
      "Iteration 9620 : Loss 3135.8724\n",
      "Iteration 9630 : Loss 3135.6465\n",
      "Iteration 9640 : Loss 3135.4208\n",
      "Iteration 9650 : Loss 3135.1953\n",
      "Iteration 9660 : Loss 3134.9701\n",
      "Iteration 9670 : Loss 3134.7451\n",
      "Iteration 9680 : Loss 3134.5204\n",
      "Iteration 9690 : Loss 3134.2959\n",
      "Iteration 9700 : Loss 3134.0717\n",
      "Iteration 9710 : Loss 3133.8477\n",
      "Iteration 9720 : Loss 3133.6240\n",
      "Iteration 9730 : Loss 3133.4005\n",
      "Iteration 9740 : Loss 3133.1772\n",
      "Iteration 9750 : Loss 3132.9542\n",
      "Iteration 9760 : Loss 3132.7315\n",
      "Iteration 9770 : Loss 3132.5090\n",
      "Iteration 9780 : Loss 3132.2867\n",
      "Iteration 9790 : Loss 3132.0646\n",
      "Iteration 9800 : Loss 3131.8429\n",
      "Iteration 9810 : Loss 3131.6213\n",
      "Iteration 9820 : Loss 3131.4000\n",
      "Iteration 9830 : Loss 3131.1789\n",
      "Iteration 9840 : Loss 3130.9581\n",
      "Iteration 9850 : Loss 3130.7375\n",
      "Iteration 9860 : Loss 3130.5172\n",
      "Iteration 9870 : Loss 3130.2971\n",
      "Iteration 9880 : Loss 3130.0772\n",
      "Iteration 9890 : Loss 3129.8576\n",
      "Iteration 9900 : Loss 3129.6382\n",
      "Iteration 9910 : Loss 3129.4191\n",
      "Iteration 9920 : Loss 3129.2002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9930 : Loss 3128.9815\n",
      "Iteration 9940 : Loss 3128.7631\n",
      "Iteration 9950 : Loss 3128.5449\n",
      "Iteration 9960 : Loss 3128.3269\n",
      "Iteration 9970 : Loss 3128.1092\n",
      "Iteration 9980 : Loss 3127.8917\n",
      "Iteration 9990 : Loss 3127.6745\n",
      "Iteration 10000 : Loss 3127.4575\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 학습하기\n",
    "losses = []\n",
    "\n",
    "for i in range (1, 10001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i,L))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8478cc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2921.263941405669\n"
     ]
    }
   ],
   "source": [
    "# test 데이터에 대한 성능 확인하기\n",
    "prediction = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f06e9877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqvklEQVR4nO2df5RcVZXvPzud7tBRJg0kkJ+QgBEWv390kDfJm1EyGhSByGBAZxR/sMJ6IjwcV7AZHYg8Z9EmM4LooDLBJ7xRIQ8wNDAOYoI6MKOkmyBElCFAeEknkA6QiKZJd7r3++PeTqqq7+26Vff3rf1Zq1dXnbq37qlTt/Y553v23kdUFcMwDKNYjEu7AoZhGEb0mHE3DMMoIGbcDcMwCogZd8MwjAJixt0wDKOAjE+7AgCTJ0/W2bNnp10NwzCMXNHT07NTVad4vZYJ4z579my6u7vTroZhGEauEJGX/V4zWcYwDKOAmHE3DMMoIGbcDcMwCogZd8MwjAJixt0wDKOAZMJbptFZs6GXlQ8/x7Zd/Uxva2XZomNZfNqMtKtlGEaOMeOeMms29HLtfc/QPzgEQO+ufq697xkAM/CGYdSNyTIps/Lh5/Yb9hH6B4dY+fBzKdXIMIwiYMY9Zbbt6q+p3DAMIwhm3FNmeltrTeWGYRhBMOOeMssWHUtrc1NZWWtzE8sWHZtSjQzDKAK2oJoyI4um5i1jGEaUVDXuInIQ8Atggnv8Pap6vYjMAe4CDgN6gI+p6oCITADuBM4AXgMuVtXNMdW/ECw+bYYZc8MwIiWILLMXOFtVTwFOBc4RkbOArwI3qeo7gDeAT7vHfxp4wy2/yT3OMAzDSJCqxl0d/uA+bXb/FDgbuMctvwNY7D6+wH2O+/pCEZGoKmwYhmFUJ5DmLiJNONLLO4B/Al4AdqnqPveQrcCIrjAD2AKgqvtEZDeOdLOz4j2XAksBjjzyyHCfwjByikUnG3ERyLir6hBwqoi0AT8Cjgt7YVW9DbgNoL29XcO+n2HkjTSjk61TKT41uUKq6i7gUeC/AW0iMtI5zAR63ce9wCwA9/VJOAurhmGUkFZ08kin0rurH+VAp7JmQ2/Vc438UNW4i8gUd8SOiLQC7wV+i2PkL3IPuxS4333c5T7HfX2dqtrI3DAqSCs62VJeNAZBZJlpwB2u7j4OWK2qD4rIs8BdIvIVYANwu3v87cD/EZFNwOvAJTHU2zByz/S2Vno9DHnc0cmW8qIxqGrcVfVp4DSP8heBMz3K3wI+HEntDKPALFt0bJnmDslEJ6fVqRjJYukHDCMlFp82gxsvPIkZba0IMKOtlRsvPCn2hU1LedEYWPoBw0iRNKKTLeVFY2DG3TAaEEt5UXxMljEMwyggZtwNwzAKiMkyhmHEhkXCpocZd8MwYsE2f08Xk2UMw4gFi4RNFzPuhmHEgkXCposZd8MwYsE2f08XM+6GYYzJmg29zO9cx5yOh5jfuS5w9kiLhE0XW1A1DMOXMIuiFgmbLmbcjcxgbnPZY6xF0SDfTWEiYZ9eDWtvgN1bYdJMWHgdnLwk7VqNiRl3IxOY21w2sUVRHMP+wFUw6H7m3Vuc55BpA2+au5EJzG0um9iiKM6IfbCiMxvsd8ozjBl3IxPYCDGb2KIojhRTS3lGMONuZAIbIWaTtHLOZ4pJM2srzwimuRuZIK1diYzqFGZRtF4WXleuuQM0tzrlGcaMu5EJzG2udvLqXZS7eo8smubMW0ZUNe060N7ert3d3WlXwzByQ6V3ETgznaxLJnmtd1YRkR5Vbfd6zTR3w8ghefUuymu984gZd8PIIXn1LsprvfOIae45J3f6pREJ09ta6fUwiFn3LsprvfOIjdxzzIh+2burH+VAVGfQxE5Gfsmr/3le651HzLjnGNMvG5e8+p/ntd55xGSZHGP6ZWOTV//zvNY7b5hxzzGmXyaPrXEYecFkmRxj+mWy2BqHkSeqGncRmSUij4rIsyLyGxH5n275chHpFZGn3L8PlJxzrYhsEpHnRGRRnB+gkTH9MllsjcPIE0FkmX3A51X1SRE5GOgRkUfc125S1X8oPVhEjgcuAU4ApgM/FZF3qmr5r8KIBNMvk8PWONLHZLHgVB25q+p2VX3Sffwm8FtgrNa8ALhLVfeq6kvAJuDMKCprGGlimSvTxWSx2qhJcxeR2cBpwK/cos+KyNMi8l0ROcQtmwFsKTltKx6dgYgsFZFuEenu6+urveaGkTC2xpEuJovVRmDjLiJvB+4FrlbV3wPfAo4BTgW2A/9Yy4VV9TZVbVfV9ilTptRyqmGkgq1xpIvJYrURyBVSRJpxDPv3VfU+AFV9teT1fwYedJ/2ArNKTp/plhlGosShz9oaR3qY629tBPGWEeB24Leq+rWS8mklh30I2Og+7gIuEZEJIjIHmAs8EV2VDaM6ps8WD5PFaiPIyH0+8DHgGRF5yi37W+AjInIqoMBm4HIAVf2NiKwGnsXxtLnCPGWMpBlLn7WRdz6xDV1qo6pxV9XHAPF46V/HOOfvgb8PUS/DCIXpsxnm6dV172pkslhwLP2AUUhMn62dRHzIn15dvh/p7i3Oc8j8tnV5w9IPGIVk2aJjuajlP3is5SpenPBRHmu5iota/sP0WR8SW6NYe0P5RtPgPF97Q7TXMWzkbhSTxU2P88HmVYwfeguAmbKTzqZVjG86BbARYiWxrVFUSjC7t3gft3tr/dcwPDHjHiMWKp0ia2/Yb9hHGD/0lmNoEpj+5+27j2WNwkuCQXB8MCqYNLP+6xiemHGPicpd3kemuUCmf+SFwW8kmMAIMY/ffSxrFF4SDMooA9/c6iyqGpFimntMWKh0yviNBBMYIebxu4/Dh1x9OlJFYdIsQJz/592S+cXUNRt6md+5jjkdDzG/c10u4iVs5B4T5oqXMguvK5cEILERYh6/+zh8yF9lMlMZnTfqVaYw9XMbPc7IJnmciYEZ99gwV7yUGRkJ1ulPHYa8fvdR+5DfOPBhbmxexUQZ2F+2R1u4cfDDfD2yq8RPXgPiTJaJCQuVzgAnL4HPbYTlu5z/CU397bt36P6T99IxeBlbhyczrMLW4cl0DF5G95+8N+2q1UQeZ2JgI/fYsFDpxsW+e4dli47l2vsG6BpYsL+stbmJG3PWyeV1JiaqHm5JCdPe3q7d3d1pV8MwjIjJm0uoF5WaO7idVAbSPYtIj6q2e71mI3fDoBhGKIsUIRdMXmdiZtyNUTSaocurN4SRHHnspMy4G2U0oqHLqzdEKDwyM64Zmt9QnXrRMW8Zo4w8BuCEJa/eEHUzkhZg9xZAYfcW9t1/JY/96Fbb3KRAmHE3ymg4Q4e/10PWvSHqxiMtwPiht7iau8rKit6pFx0z7kYZDWfoaEC/dJ+0ANPltVFlRe7Ui44Zd6MMP0P3nuOm5C63RlAWnzaDGy88iRltrQgwo601E25useGTX2ebHjaqrMidetGxBVWjDC+3r/ccN4V7e3oLvciaR2+IuvHIu7Ov6SBuHr6k7LBCz14aADPuxigqDd38znWN501SZDzy7oxfeB0Lhubzn+YtUxjMuBtVacRF1sQIsVl0KE5eMuo6iynOTMwwzd0IQCMusiaCh0siD1zllGeJp1fDTSfC8jbnf9bqZ3hixt2oSsN5kyRFHjaLzksHZIzCjLtRlYbzJkmKFLcCDEweOiDDE9PcjUA0lDdJUkya6Y6IPcqzQh46IMMTM+4ZpdGSdzUkKW4FGJgYOiC7t5PBZJkMMpK8y/J8FJyTlzibQ2d5s+iF1zkdTikhOiC7t5Oj6shdRGYBdwJHAArcpqpfF5FDgbuB2cBmYImqviEiAnwd+ACwB/iEqj4ZT/XrJ8ujh4bIUujlAgjpuAWmiYdLYqaIeC/ahri3M0IQWWYf8HlVfVJEDgZ6ROQR4BPAWlXtFJEOoAP4AvB+YK779y7gW+7/zJD1tLaF9ysf8cAYkSN2b4H7rwBVGB48UPbAVc7jJIxfWv7meSDCDqjw93aGqCrLqOr2kZG3qr4J/BaYAVwA3OEedgdODARu+Z3q8EugTUSmRV3xMGQ9rW3h/cq9PDCGBg4Y9hGS8sowd7/EKPy9nSFq0txFZDZwGvAr4AhV3e6+9AqObAOO4S9dgdnqllW+11IR6RaR7r6+vlrrHYqsjx4K71dei6dFEl4Z5u6XGIW/tzNEYOMuIm8H7gWuVtXfl76mzi7bNe20raq3qWq7qrZPmTKlllNDk/XRQ+H9ymvxtEjCLdDc/RKj8Pd2hgjkCikizTiG/fuqep9b/KqITFPV7a7sssMt7wVmlZw+0y3LDMsWHeu5m3mWRg+F9iv3cgFsainX3CE5t8A8+JsXiELf2xmi6sjd9X65Hfitqn6t5KUu4FL38aXA/SXlHxeHs4DdJfJNJrDRQ8p4uQBe8E+w+NZ03AIjdvczjCwgjqIyxgEiC4B/B54Bht3iv8XR3VcDRwIv47hCvu52Bt8EzsFxhfykqnaPdY329nbt7h7zEKMGsuzmmVmK7C1T5M/W4IhIj6q2e75WzbgngRn36Kh08wRHcrKZScJkxaBWup2CMyvJWrCUURdjGXdLPxAnCfzAK0fpewb2WZBI2nj58Sfps1/KWJ5AeYwfyEqnmQPMuMdFAj9wr2AsP2py80yhUyqUdJS2QS0lTU+gqH8DWeo0c4AZ97iI4QdeaRD/uHf0KN2PwG6eKXVK1973DDO2PMi8F76R/1FZllwr0/QEivo3kKVOMwdY4rC4iPgH7pVwaVf/YNXzoEY3zwQCerwihN879HNOfPLvihEl6mc4wxjUendDStMTKOpOLkudZg4w4x4XtfzAA/xwvQyiH22tzfW7eSbwA/KSiK4Zv5pW9pYX5jVKNGqDGiY9QpqZJ6Pu5OLoNAuMyTJxETRXd0AZJKhm3trcxPLzT/A05uu7vsOsJ1dyuPaxQ6aw5fRlzDv/8vKDEpjGT29rHbU+MF12eh+cx1FZxJkUfWdTP/5CsGuklXky6nz1Kea/z+MakY3c4yLoiCmgDOKnmR8yMdgofX3Xdzix50tMpY9xAlPp48SeL7G+6zvlByYwjffKL7Kdyd4H53VUdvIS+NxGWL7L+R/GuPp1cP2vZ1vGinrWkNIsJK856M3PPW2Wt+Gdlkccw+AS1n/9leXvYCqjE7S9whSmLt9UXphArvXKkdDNxz/PvGeuN39sL2460Xs25cWkWU5nYkTG/M51np5oM9paebzj7BRqdADzc88yAWWQEQNe79TwcO0D8Sr3kEMqp/ExeNCMzi9yNsw+xHyYvfCSI/zIo4yVcbbt6uf8cY9xzfjVTJedbNPJrNi3hAd2LUi7amNixj1tatARwyRc+r0cTBtvepS/nbZqJ4d1QQvqN5/1XYnSwkvDH/ijI8tUklcZK8Nc+vYnuGZwFRNlAICZspPO5lUc2twCnJtu5cbANPekqfSMgUR0xEqNu1p5GWE8aGwjjHg44UPB10bqdaM0ALim+e79hn2EiTLANc13p1SjYNjIPUn85I3zboldJ50wuLum8jLCeNBY4El4vO6bX/8ATvkoPP+TsWdEOYnqzLI3ysT+V2oqzwo2ck+SNHf8CeMjHMaDxgJPwuN33zz/k+oeOTnYZSrz3ig59a83454kKRq69cdcSb+2lJX1awvrj7my+slhXNBy+sPIFGHuG99zt2RGqsn6nsZ5zfdvskySpJjn4+pn53LG4GXuiv9rbNPDWLFvCT3PzuXx8wO8QZ2LneuPuZITe75Ea4lm2a8tPDDpk3y9c10mp+GZI8x943cucqA8Zakm894oUQeluQQKKgyBGfckSTHCbtuufnpZQNdA+Q9GYt4U3K9TeeC/jkNxrj0yDQfMwHsR5r7xdKMURsVWpLgOkgtvlIg9uUaCCltlANygwkk9X2I9RGbgTZZJkhTzfKS1Kfi2Xf10DS9gwcAtHL33+ywYuIWu4QWjwrYyNQ3PGmHuG69z/fayT0mqyZw3SgLeRbOeXFk2mwVolQFmPbkysmvYyD1pUvLlTmtTcK88Mn7UlHM+SySR/35oPiv33sK2t/qZflAry4aOZXHQkyvvua/O8faRh1Skmkx5oyTkXVRTUGGd2Mi9QUhrU3CvPDIe9zQQ/ywiFhLw40/NmyQPnlxRk5B30Q6Z4lPuk2OpDsy4NxCLT5vB4x1n81LnuTzecXYi+rZXp/JXZx05yuAnMYuIhZTy34eSsfrfCH6sn7dNlNJFlrxREvJo23L6Mk/vtS2nL4vsGibLGLHjlTah/ahDMxu0UhMp5b8fq7wqvh40PsdWErV0EZM3Sl0k5NE27/zLWQ+ut8xOdshktpxh3jKRkOWIuEYgTJ6cTJFS/vuR8rrw8qAZ1wwiMFSyyOc3eo4j6jgreYUS9Gibd/7l4Brzqe5flDSkLJP5iDgjP6SU/z6UjOXlQbP4VjjtYyDudaTJSW/gZXCLHHWc5s5VEdOQI/exNMxCjCaN5EhAUgib7tmLSu+bmzc/z7xnfgDq/i50yMlfc+RZoz9LmptuJ0FWZhEhaUjjHrmGaTQ2CRiDKGWsNRt6eexHt3I3dzF9wk627ZnMxJ63QAJKLWGliwRcR40GNe6Ra5hGISnqusxTD93GDXJbWUSo74ZsXlJLmNlKTrJUFoGGNO5pBfQY+aFyW8M8p0io7KTuHvgXJo4rj44Uv+ADP6ml3tmKpYBOjKoLqiLyXRHZISIbS8qWi0iviDzl/n2g5LVrRWSTiDwnIoviqngY0groMfJD5jMVBsTLeWC6eEdBjhq8x+ElUuTF2IwRZOT+PeCbwJ0V5Tep6j+UFojI8cAlwAnAdOCnIvJOVR0iYxTGFc+IhaKsy3h1Utt0MjM9DPxAcxsTJh4crxZe9MXYDFF15K6qvwB8ElGM4gLgLlXdq6ovAZuAM0PUzzBSIa1Ea1Hj1Rmt2LeEPRXRkfuaDmLCeSurb/4RlixFoxacMH7unxWRp13Z5hC3bAZQ2i1vdctGISJLRaRbRLr7+vpCVKOBsL0wEyNy3/KU8OqMuoYXsKL5M2W+3OMv+EYymneB/MizjqjvMnnJQSKzgQdV9UT3+RHAThyZ7n8B01T1UyLyTeCXqvov7nG3Az9W1XvGev/29nbt7u4O9UEKR6W72Nz3OX7Hle5nM8+EzY85fsnSBGd8Aj74tdSqXSSK4C1TuTAMTidla0zFQER6VLXd67W6vGVU9dWSN/9n4EH3aS8wq+TQmW6ZUQte7mLd38Vzg4WXfn7guQ5B9+3OYzPwoSnCukwcAVBGPqjLuIvINFXd7j79EDDiSdMF/EBEvoazoDoXeCJ0LRsNL3cxvw0WvOj5XvaNe14DWXJY7yJ0UkbtVDXuIvJD4N3AZBHZClwPvFtETsWxOJuBywFU9Tcishp4FtgHXBGXp0wRpsy+hHULy55zUjlZDGR58G+cTnEseauWekfcCRT6fjdiIZDmHje1au6F1xFvOtF/U+OgI/jlu6OsUbT4fb5JsxwvjaR58G8OyFmltH+63MAHrXdlJwDO+kidC4eFv9+NuhlLc89lVsiiBJj44ucu1v6pir0wc0pSgSxBvYt6vhesPGi9I97Ao/D3uxELuUw/UJQAE1+C5u7w2wuz9dD46xiGJAJZapFQ/GSsyvKg9Y648yr8/W7EQi6Ne0Mk/spr2tEgWnMMGyJUatKPyHVMDJrDRJq8DbyU+7m/0Dafo3fdVZaHRRVebJvPMaUH+nUCrYe40k5tOnxD3O9G5ORSlilKgElo/PbCrGWPzCgJull0xIEsXvlTDtrzivfBXqPnMz7hfWxF+dteXjsqwZaIU16Gl6w2rhkG/lDXRtp2vxv1kMuRu/nuuiSZpyPIiLyWjH8Rzky886cc5pk/xbNtRhZNq3jLHK59zpp2BYdrxXW8ZLWBP46W0AJmQ7T73aiHXBp3MN9dILn9HoPq1yll/PPLn9LZvGp/znJg7Lb54NeqxgbskClMZXSqjB0yefT+l5Wd1/I27zcN2DaLmx5n8YQb4KCtMGEmNF0H5FC2MxIjl7KM4XLyEl6YfgH7GIcq7GMcL0y/IHqtPqj3h9+MIeaMf0Hzp4TNYbLl9GX0VyTc6tcWtpy+rPrJYdomqNxlGCWYcc8x67u+w/TN9zGeYURgPMNM33wf67u+E+2Fgo7IU8r456dJn3ru0kizHM47/3I2nvEVXmEKwyq8whQ2nvEVZxf7aoRpm4hdK43GILeyjAGznlxJq5TvqNMqA8x6ciUEMTgB2dM6lYn9273LSwsS2CzaiyQ16XnnX76/bae6f4E4eQn8v1+W6/qnfDRY29gGF0YdmHHPMYEX+EKyYvBirtFby/TrPdrCisGLWV55cEounJlfg3l6tZPVc8TlUoec50eeVb29bIMLow5MlskxO2SKT/nkSK9zxx/OpGPwMrYOT2ZYha3Dk+kYvIw7/mD7sAQmjLTSCBtc2F4FkWMj9xyz5fRlTOr5Upk0068tbDljWXC5IADT21rp2rWAroEFZeUzIg6iKXRyrDDSSkpyV2JkMZFcATDjnmPmnX8563G098N1JztkMlvOWBZsga8Gli061jNxVZRBNJXJsXp39XPtfc8AFMPAh5VW8hqxHIRa4iOMwJhxzzmeC3wRp5tNYsFyrORYhTDuScUk5BFbMI4FM+5FI6YpbtwLlqGTY8WxiUbQ9wxyXMaklUxJYLZgHAtm3Oshy7vx5HSKGyo5VhwdWtD3rOXaGZFWMieB2awmFsxbplayHi2Y0yluqORYcQT5BH3PHAYYZS4/fMSJ5AwHG7nXStZHxnmZ4lbMfhYvvA4unF+fVBBHhxb0PcNeO4VZYCbzw2dkVlMkCmXcE9ERsz4yzsMU10fKWHzKR1k84Se1J8eKo0ML+p5hrp2SC6Dlh28MCiPLeOX0vva+Z1izoTfaC6WUHCsweZji+s1+ur9bn9wVR5BP0Pf0OW79MVcyv3MdczoeYn7nOu/7MCVJx/LDNwaFGbkn5kqXh5Fx1qe4vrOcis2/g8pdcXiiBH1Pj+PWH3MlH19/FP3uPeK7YJnSLDAK19ZMedsYnhTGuCemI2bMpS2X+EkZXgQ1dHF0aEHfs+K4qzvX7TfsI3gONFJcHwnj2po5bxvDk8IY90R1xKyPjLOO1+wHYdTIHbIjd9WA34Cid1c/8zvX7R/t3nz8lcx75vpszwI9iGOWbDOB6CmM5m46YsxEmdjJa12g/VOFSY7lN6AQKFsT+vj6o1h/0pezvT7iQdSz5MTWyxqMwozcbZ/JGInDq8Nr9nPkWYWQu7xy8XjNS/oHh7j62bk83rGxvgulFEwX9Sy58KknUiK/xt3jxl7cRLh9JrMceZomSfn2F0Tu8hpoeBlDCLEmlKQbZcXv4ubjRxaMo0kkl0m/+wKQT+PudWPffwWowvDggbJabnZLO+pP1n37M0jlguX8znXRrgkl1eF6/C7mPXM9d877Mlc/OzeSWbL53cdDVc1dRL4rIjtEZGNJ2aEi8oiIPO/+P8QtFxG5RUQ2icjTInJ6LLX2urGHBg4Y9hFq8RlOyOd4zYbe6v7PWSPrvv054D3HeW+s4ldelaQ6XJ/fxbwXvsHjHWfzUue5PN5xdij5xNbL4iHIgur3gHMqyjqAtao6F1jrPgd4PzDX/VsKfCuaalZQyw0c9Fif43T31siMcW4XjhphJ6CYefR3fTWVVyWpDjeBTmTxaTO48cKTmNHWiuBsAnPjhSeZ3h6SqrKMqv5CRGZXFF8AvNt9fAfwM+ALbvmdqqrAL0WkTUSmqero3ZXDUIufdNCb3ec9t+lh+6eMYf15c7twZL79NVPp2he55p5UMF1CvviZ3wM3h9SruR9RYrBfAY5wH88ASu+ErW7ZKOMuIktxRvcceeSRtV3d68ZuainX3KG2m93jPfuZwFcHyw1YGGMcx8JRWP/gwOcXZLHTlwgX072CfHy8+OvXlZPqcPMQkW14EnpBVVVVRLzu22rn3QbcBtDe3l7b+X43tldZ0Jvd4z07+s6ja3jBqEPrNcZRLxyFjRRMM9IwU0ErES+me83QlNHukKF15SQ6XJu15ZZ6jfurI3KLiEwDdrjlvcCskuNmumXR43djh7jp1gzNZ+XeW9j2Vj/TD2plz0H7YM/gqOPqNcZR70UaVuap5fwojXHmwtcj9jzZtquf88c9xjXjVzNddrJNJ7Ni3xK6hhcwo601Gx1aLRR91lZQ6jXuXcClQKf7//6S8s+KyF3Au4DdkevtMeFlcJrHCc1NwuDQgfFWGGMcdaBVWJkn6PlRG+PMrT1EvGh46duf4JrBVUyUAQBmyk46m1dxaHMLyzu+XG8tDaMmqhp3EfkhzuLpZBHZClyPY9RXi8ingZc5ECn0r8AHgE3AHuCTMdQ5FrwMzuCw0tbazNsmjI9stBXlwlFYmSfo+VEb48wFrYRcNKyc1TwoP9xv2EeYKANc03w3YMbdEwsgjJwg3jIf8XlpocexClwRtlKJUHEztf/+PHoZra/v7h/kqevfl0IFqxNW5gl6ftTGuJZOKRFtfuF17Lv/SsYPvbW/aF/TQYwPsGjoNauZNGGHI7BXMLH/lciqHJasrXmUtf/uLc5zMAMfgsIkDqsJj31QO1tu5/xxj406NMtRcmH9g4Oe79cGYdYeggStJBUXsGZoPh2Dl7F1eDLDKmwdnkzH4GWsGZpf9VyvWc02Pcz74IwEfWUt3mLPj68r61gBxg+9xZ4fm0dOGPKZfiAsHgtorezlC82r6dp7YPSehyi5sDJPkPOXLTqWx350K1dz1/4Fwpu5hAWLPlP3NaH62kNS2vzKh5+jd+BPuYc/LSv/zwDX8Zq9rNi3hM7mVeXSTAzug/WOvrO25nGQz4zGr9wIRmMad5+FsunyWj69GWJmcdPjfLB51f7R1UzZSWfTKsY3nUJNidlK3zNAp5KUNh/mOl4SU9fwAmfx9G33xqYhh1nkztqax7bhw5g5bqd3eQr1KQqNKcv4TI9l0szI8mUUirU3eE6b497rM2o5KI7rLFt0LM1N5QJ7c5Nw6rlL4XMbYfku53/E2vFYo+9qJNWuQVnV8tfs0Zaysj3awqqWv06lPkWhMY275UqpjZSyQiaVUCp0Uq/KELyaQ/pqJ8zoO2uJuk49dynX6dKyNY/rdKnTQRp105iyjEXd1UZKe30mtQFLmKReKx9+jsHhcms+OKyx69dh3GCztrGNc93PcPHDCzNRn6LQmMYdLOquFuLILxLQrzmJhFJhRsFp6ddh3WCzlqgra/UpAo0pyxi14bXnaZi9Pj1cUXngquD7ska5nyvhNOi09GtLk2tUQ5y4o3Rpb2/X7u7utKthJMVNJ/rIPLOcxcexqEzyBc4sIkRnU+l5As4oOIixDHNuUmQqYMmIFBHpUdV2r9ds5G4kT5gF2hh2zAozCs76CDprAUtGcjSu5m65LMJTbxu2HgL9r3uXVyMmz50wmm+W9eKsBSwZydGYxt02ww5PUm1Y2YH4dQwZCe2vmZgHGVkLWDKSozGNe1I7xxeZMG3Y/4Znsfa/zqvL38Hh2scOmcIfj1rIMdvuL+9AmlpgXHP9O25liQQSZkW9QYyRHxpTc69lah+xZ0ZhCCGP7Gmd6lmuClPpY5w4/+dsvmt0BzI0ABMOjs5zJ0WSSJiVtYClPLBmQy/zO9cxp+Mh5neuy+36RGOO3IMG5fhID+s3v8HVz85tbO+DEIFNKwYv5hq9tSyx1rDCuIo0uZXP99P/BnzhpRoqm01qSZhVr8dL1gKWsk7mdgkLQWMa96BBOT7Sw/SeFfTuvQXI95cP1K/5+rTh+mOu5OrOdWMakjv+cCavjxtwt6F7jW16GDNkdOIoX/Kqr1cQNGFWWIOT5QXfrFGkBejGlGWCBuX4SAzTeK3sedCETZkjTDCRRxuuP+nLfHz9UVXd7qa3tdI1vIAFA7dw9N7vs2DgFnp1sudlhisL8qqvexA0YVaYJGFGbRRpAboxjTs4xqla1j6fEaLXZgx5/PJD+4xXtOHVz84NZIS8dOB/HL54lKHr1xZeOuqSQujrXgRNmJW2wSmKBh2ErGXMDENjyjJB8ZAe+pnAin2jjUsev/yofcaDGiEvHfjPF13BgxsOY/7LtzKN19jOYTx+1GdY8snP11WXPLD4tBl0v3wJf/6r+Qyp0iTCR941y3MnrLQ8XoqkQQchbM6eLGHGfSw8skduPOZKHll/FAxn+MsPqqNHnO2xFiNUqQOv2dDL9S+dQP/gLfvLWl9qomVDbyGNCDif+d6eXobcFCBDqtzb00v7UYeWfeY0DU6RNOggFGkB2ox7NSqyR84DbpyVnVwdlV4UNx//PPOeuT5YcFHIbI+V137PcVO4t6e3LiPUaEYEgn/mNA1O2pJQGhRlAdqMex1k5cv3mjJP71kBEjC4KERee69r39vTy1+eMYNHf9dXsxFqRCNSy2dO656zIKj8YsY9x3iN/Kbh41Lop6PXmdfeb9T56O/6eLzj7JrfrxGNSB4+c5E06Eajcb1lCoDXCG+bj0th1L7hUY+0GzGSMg+fOetZLw1/bOSeY7xGfiv2LeGrLbfTyt4DhTH4hkc96izSQlZQ8vKZsyJDGrVhm3XkGL+NIu6c9zLzXvhGrOmM87BJhWEUnbE267CRe47xG/nNO+0c4PJIr+WV2+TGC09KZdRpOwsZRnVCjdxFZDPwJjAE7FPVdhE5FLgbmA1sBpaoqneOVxcbuWebLI3Ss1QXw0ibuLfZe4+qnlpygQ5grarOBda6z40ck6XcJlmqi2FkmThkmQuAd7uP7wB+BnwhhusYPkQtW2TJBz2uupjUYxSNsCN3BX4iIj0iMpLt6AhV3e4+fgU4IuQ1jBqIY0PkLCVTiqMutom0UUTCGvcFqno68H7gChH5s9IX1RH0PUV9EVkqIt0i0t3X1xeyGsYIccgWWfLHjqMuJvUYRSSUcVfVXvf/DuBHwJnAqyIyDcD9v8Pn3NtUtV1V26dMmRKmGkYJccgWWQpkiaMuWZKdDCMq6tbcReRtwDhVfdN9/D7gBqALuBTodP/fH0VFjWDEFdKepUCWoHUJqqPnIQ2AYdRKmJH7EcBjIvJr4AngIVX9Nxyj/l4ReR74C/e5kRBZklDSpBYd3drMKCJ1j9xV9UXgFI/y14CFYSpl1E9eQtrjppYUwtZmRhGxCNUCkiUJJS1q1dGtzYyiYcbdSIW4/cpNRzcaHUv5ayROEn7lpqMbjY4ZdyNxkvArz5L7pmGkgckyRuIk5VduOrrRyNjI3UicLKUzMIyiYsbdSBzTww0jfkyWMRLH/MoNI37MuBupYHq4YcSLyTKGYRgFxIy7YRhGATHjbhiGUUDMuBuGYRQQM+6GYRgFRJyd8FKuhEgf8HICl5oM7EzgOnnC2sQbaxdvrF28SatdjlJVz63sMmHck0JEulW1Pe16ZAlrE2+sXbyxdvEmi+1isoxhGEYBMeNuGIZRQBrNuN+WdgUyiLWJN9Yu3li7eJO5dmkozd0wDKNRaLSRu2EYRkNgxt0wDKOAFMq4i8ihIvKIiDzv/j/E57h/E5FdIvJgRfkcEfmViGwSkbtFpCWZmsdLDe1yqXvM8yJyaUn5z0TkORF5yv07PLnaR4+InON+nk0i0uHx+gT3+9/k3g+zS1671i1/TkQWJVrxmKm3XURktoj0l9wf30688jERoE3+TESeFJF9InJRxWuev6fEUNXC/AErgA73cQfwVZ/jFgLnAQ9WlK8GLnEffxv4H2l/pqTaBTgUeNH9f4j7+BD3tZ8B7Wl/jojaogl4ATgaaAF+DRxfccxngG+7jy8B7nYfH+8ePwGY475PU9qfKQPtMhvYmPZnSKlNZgMnA3cCF5WU+/6ekvor1MgduAC4w318B7DY6yBVXQu8WVomIgKcDdxT7fwcEqRdFgGPqOrrqvoG8AhwTjLVS5QzgU2q+qKqDgB34bRPKaXtdQ+w0L0/LgDuUtW9qvoSsMl9vyIQpl2KStU2UdXNqvo0MFxxbuq/p6IZ9yNUdbv7+BXgiBrOPQzYpar73OdbgaLsJhGkXWYAW0qeV37+/+1Ouf8u5z/oap+z7Bj3ftiNc38EOTevhGkXgDkiskFEfi4i/z3uyiZEmO879XsldzsxichPgakeL32x9Imqqog0jJ9nzO3yV6raKyIHA/cCH8OZhhoGwHbgSFV9TUTOANaIyAmq+vu0K9bI5M64q+pf+L0mIq+KyDRV3S4i04AdNbz1a0CbiIx3RyUzgd6Q1U2MCNqlF3h3yfOZOFo7qtrr/n9TRH6AM13Nq3HvBWaVPPf6nkeO2Soi44FJOPdHkHPzSt3too7IvBdAVXtE5AXgnUB37LWOlzDft+/vKSmKJst0ASOr0pcC9wc90b1BHwVGVrxrOj/jBGmXh4H3icghrjfN+4CHRWS8iEwGEJFm4IPAxgTqHBfrgbmuZ1QLzsJgV8Uxpe11EbDOvT+6gEtcr5E5wFzgiYTqHTd1t4uITBGRJgARORqnXV5MqN5xEqRN/PD8PcVUT2/SXpGOeHX7MGAt8DzwU+BQt7wdWFVy3L8DfUA/jha2yC0/GufHugn4v8CEtD9Twu3yKfezbwI+6Za9DegBngZ+A3ydnHuIAB8A/gvHE+KLbtkNwPnu44Pc73+Tez8cXXLuF93zngPen/ZnyUK7AH/p3htPAU8C56X9WRJsk3muDfkjzuzuNyXnjvo9Jfln6QcMwzAKSNFkGcMwDAMz7oZhGIXEjLthGEYBMeNuGIZRQMy4G4ZhFBAz7oZhGAXEjLthGEYB+f+aBqy6eaYnpwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 정답 데이터와 예측한 데이터 시각화하기\n",
    "plt.scatter(X_test[:, 0], y_test)\n",
    "plt.scatter(X_test[:, 0], prediction)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc471c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
